{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54cf468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "class Q_learning_grid:\n",
    "    def __init__(self,N :int,goal : str):\n",
    "        if N<2:\n",
    "            print(\"N should be greater than or equal to 2\")\n",
    "            exit(0)\n",
    "        self.N = N\n",
    "        self.goal = goal.lower()\n",
    "        self.construct_Environment()\n",
    "        self.construct_rewards_initial_Q()\n",
    "\n",
    "    def construct_Environment(self):\n",
    "        #n = int(input(\"Enter N:\"))\n",
    "        self.environment = {}\n",
    "        k = 1\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                self.environment[(i,j)] = f\"s{k}\"\n",
    "                k+=1\n",
    "        print(self.environment)\n",
    "            \n",
    "    def return_index(self,s : str):\n",
    "        for k in self.environment:\n",
    "            if self.environment[k] == s:\n",
    "                return k\n",
    "\n",
    "    def action_reward(self,action : int, i_p : int , j_p : int):#returns reward and next state index\n",
    "        if action==0:\n",
    "            i_n = i_p-1 #i_new\n",
    "            if i_n<0:\n",
    "                return [-100,i_p,j_p]#i_previous\n",
    "            elif self.environment[(i_n,j_p)] == self.goal:\n",
    "                return [100,i_n,j_p]\n",
    "            else:\n",
    "                return [0,i_n,j_p]\n",
    "        elif action == 1:\n",
    "            j_n = j_p+1\n",
    "            if j_n>=self.N:\n",
    "                return [-100,i_p,j_p]\n",
    "            elif self.environment[(i_p,j_n)] == self.goal:\n",
    "                return [100,i_p,j_n]\n",
    "            else:\n",
    "                return [0,i_p,j_n]\n",
    "        elif action == 2:\n",
    "            i_n = i_p+1\n",
    "            if i_n>=self.N:\n",
    "                return [-100,i_p,j_p]\n",
    "            elif self.environment[(i_n,j_p)] == self.goal:\n",
    "                return [100,i_n,j_p]\n",
    "            else:\n",
    "                return [0,i_n,j_p]\n",
    "        elif action==3:\n",
    "            j_n = j_p-1\n",
    "            if j_n<0:\n",
    "                return [-100,i_p,j_p]\n",
    "            elif self.environment[(i_p,j_n)] == self.goal:\n",
    "                return [100,i_p,j_n]\n",
    "            else:\n",
    "                return [0,i_p,j_n]  \n",
    "\n",
    "    def construct_rewards_initial_Q(self):    \n",
    "        self.rewards = {}\n",
    "        k = 1\n",
    "        i = 0\n",
    "        j = 0 \n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                self.rewards[f\"s{k}\"] = [\n",
    "                    self.action_reward(0,i,j)[0],#north\n",
    "                    self.action_reward(1,i,j)[0],#east\n",
    "                    self.action_reward(2,i,j)[0],#south\n",
    "                    self.action_reward(3,i,j)[0]#west   \n",
    "                ]\n",
    "                k=k+1\n",
    "\n",
    "        self.Q_table = copy.deepcopy(self.rewards)\n",
    "        print(self.rewards)\n",
    "\n",
    "    def make_learn(self,alpha=1.0,gamma=0.9,epochs = 20):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        start = \"s1\"\n",
    "        if (start == self.goal):\n",
    "            start = \"s2\"\n",
    "        i = 0\n",
    "        for i in range(20):\n",
    "            state = start\n",
    "            while state != self.goal:\n",
    "                action = random.randint(0,3)\n",
    "                rtplus1 = self.rewards[state][action]\n",
    "                old_Q = self.Q_table[state][action]\n",
    "\n",
    "                (a,b) = self.return_index(state)\n",
    "                after_action = self.action_reward(action,a,b)\n",
    "                if after_action[0] == -100:\n",
    "                    next_maxQ = 0\n",
    "                    self.Q_table[state][action] = rtplus1\n",
    "                    continue\n",
    "                next_state = self.environment[(after_action[1],after_action[2])]\n",
    "                # print(f\"{state}-->{next_state}\")\n",
    "                next_maxQ = max(self.Q_table[next_state])\n",
    "                # print(next_maxQ)\n",
    "                new_Q = ((1.0-self.alpha)*old_Q )+ (self.alpha*(rtplus1+(self.gamma*next_maxQ)))\n",
    "                self.Q_table[state][action] = new_Q\n",
    "                state = next_state\n",
    "                # print(rewards)\n",
    "            # print(self.Q_table)\n",
    "        print(self.Q_table)\n",
    "        \n",
    "    def create_policy(self):\n",
    "        policy = {}\n",
    "        # print(Q_table)\n",
    "        for state in self.Q_table:\n",
    "            policy[state] = self.Q_table[state].index(max(self.Q_table[state]))\n",
    "        print(policy)\n",
    "        return policy\n",
    "    \n",
    "    def construct_path(self,start):\n",
    "        start = start.lower()\n",
    "        path = \"\"\n",
    "        policy = self.create_policy()\n",
    "        state = start \n",
    "        path = path+f\"{state}\"\n",
    "        while state!=self.goal:\n",
    "            action = policy[state]\n",
    "            a,b = self.return_index(state)\n",
    "            next_state_index = self.action_reward(action,a,b)\n",
    "            next_state = self.environment[(next_state_index[1],next_state_index[2])]\n",
    "            state = next_state\n",
    "            path = path+f\"-->{state}\"\n",
    "        return(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce739be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 's1', (0, 1): 's2', (0, 2): 's3', (0, 3): 's4', (1, 0): 's5', (1, 1): 's6', (1, 2): 's7', (1, 3): 's8', (2, 0): 's9', (2, 1): 's10', (2, 2): 's11', (2, 3): 's12', (3, 0): 's13', (3, 1): 's14', (3, 2): 's15', (3, 3): 's16'}\n",
      "{'s1': [-100, 0, 0, -100], 's2': [-100, 0, 0, 0], 's3': [-100, 100, 0, 0], 's4': [-100, -100, 0, 0], 's5': [0, 0, 0, -100], 's6': [0, 0, 0, 0], 's7': [0, 0, 0, 0], 's8': [100, -100, 0, 0], 's9': [0, 0, 0, -100], 's10': [0, 0, 0, 0], 's11': [0, 0, 0, 0], 's12': [0, -100, 0, 0], 's13': [0, 0, -100, -100], 's14': [0, 0, -100, 0], 's15': [0, 0, -100, 0], 's16': [0, -100, -100, 0]}\n",
      "{'s1': [-100, 81.0, 65.61000000000001, -100], 's2': [-100, 90.0, 72.9, 72.9], 's3': [-100, 100.0, 81.0, 81.0], 's4': [-100, -100, 0, 0], 's5': [72.9, 72.9, 59.049000000000014, -100], 's6': [81.0, 81.0, 65.61000000000001, 65.61000000000001], 's7': [90.0, 90.0, 72.9, 72.9], 's8': [100.0, -100, 81.0, 81.0], 's9': [65.61000000000001, 65.61000000000001, 53.144100000000016, -100], 's10': [72.9, 72.9, 59.049000000000014, 59.049000000000014], 's11': [81.0, 81.0, 65.61000000000001, 65.61000000000001], 's12': [90.0, -100, 72.9, 72.9], 's13': [59.049000000000014, 59.049000000000014, -100, -100], 's14': [65.61000000000001, 65.61000000000001, -100, 53.144100000000016], 's15': [72.9, 72.9, -100, 59.049000000000014], 's16': [81.0, -100, -100, 65.61000000000001]}\n",
      "{'s1': 1, 's2': 1, 's3': 1, 's4': 2, 's5': 0, 's6': 0, 's7': 0, 's8': 0, 's9': 0, 's10': 0, 's11': 0, 's12': 0, 's13': 0, 's14': 0, 's15': 0, 's16': 0}\n",
      "s16-->s12-->s8-->s4\n"
     ]
    }
   ],
   "source": [
    "new_grid = Q_learning_grid(4,\"s4\")\n",
    "new_grid.make_learn()\n",
    "path = new_grid.construct_path(\"s16\")\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "173a7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = int(input(\"Enter N:\"))\n",
    "# # n= 3\n",
    "# environment = {}\n",
    "# k = 1\n",
    "# for i in range(n):\n",
    "#     for j in range(n):\n",
    "#         environment[(i,j)] = f\"s{k}\"\n",
    "#         k+=1\n",
    "\n",
    "# print(environment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "751464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_index(s : str):\n",
    "#     for k in environment:\n",
    "#         if environment[k] == s:\n",
    "#             return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da5ee83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal = input(\"Enter Goal state:\")\n",
    "# start = \"s1\"\n",
    "# # start = \"s7\"\n",
    "# # goal = \"s3\"\n",
    "# if (start == goal):\n",
    "#     start = \"s2\"\n",
    "\n",
    "# def action_reward(action : int, i_p : int , j_p : int):#returns reward and next state index\n",
    "#     if action==0:\n",
    "#         i_n = i_p-1 #i_new\n",
    "#         if i_n<0:\n",
    "#             return [-100,i_p,j_p]\n",
    "#         elif environment[(i_n,j_p)] == goal:\n",
    "#             return [100,i_n,j_p]\n",
    "#         else:\n",
    "#             return [0,i_n,j_p]\n",
    "#     elif action == 1:\n",
    "#         j_n = j_p+1\n",
    "#         if j_n>=n:\n",
    "#             return [-100,i_p,j_p]\n",
    "#         elif environment[(i_p,j_n)] == goal:\n",
    "#             return [100,i_p,j_n]\n",
    "#         else:\n",
    "#             return [0,i_p,j_n]\n",
    "#     elif action == 2:\n",
    "#         i_n = i_p+1\n",
    "#         if i_n>=n:\n",
    "#             return [-100,i_p,j_p]\n",
    "#         elif environment[(i_n,j_p)] == goal:\n",
    "#             return [100,i_n,j_p]\n",
    "#         else:\n",
    "#             return [0,i_n,j_p]\n",
    "#     elif action==3:\n",
    "#         j_n = j_p-1\n",
    "#         if j_n<0:\n",
    "#             return [-100,i_p,j_p]\n",
    "#         elif environment[(i_p,j_n)] == goal:\n",
    "#             return [100,i_p,j_n]\n",
    "#         else:\n",
    "#             return [0,i_p,j_n]\n",
    "    \n",
    "# rewards = {}\n",
    "# k = 1\n",
    "# i = 0\n",
    "# j = 0 \n",
    "# for i in range(n):\n",
    "#     for j in range(n):\n",
    "#         rewards[f\"s{k}\"] = [\n",
    "#             action_reward(0,i,j)[0],#north\n",
    "#             action_reward(1,i,j)[0],#east\n",
    "#             action_reward(2,i,j)[0],#south\n",
    "#             action_reward(3,i,j)[0]#west   \n",
    "#         ]\n",
    "#         k=k+1\n",
    "\n",
    "# import copy\n",
    "# Q_table = copy.deepcopy(rewards)\n",
    "# print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ea4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# alpha = 1.0\n",
    "# gamma = 0.9\n",
    "\n",
    "# i = 0\n",
    "# for i in range(20):\n",
    "#     state = start\n",
    "#     while state != goal:\n",
    "#         action = random.randint(0,3)\n",
    "#         rtplus1 = rewards[state][action]\n",
    "#         old_Q = Q_table[state][action]\n",
    "\n",
    "#         (a,b) = return_index(state)\n",
    "#         after_action = action_reward(action,a,b)\n",
    "#         if after_action[0] == -100:\n",
    "#             next_maxQ = 0\n",
    "#             Q_table[state][action] = rtplus1\n",
    "#             continue\n",
    "#         next_state = environment[(after_action[1],after_action[2])]\n",
    "#         # print(f\"{state}-->{next_state}\")\n",
    "#         next_maxQ = max(Q_table[next_state])\n",
    "#         # print(next_maxQ)\n",
    "#         new_Q = ((1.0-alpha)*old_Q )+ (alpha*(rtplus1+(gamma*next_maxQ)))\n",
    "#         Q_table[state][action] = new_Q\n",
    "#         state = next_state\n",
    "#         # print(rewards)\n",
    "#     print(Q_table)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8153e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_policy():\n",
    "#     policy = {}\n",
    "#     # print(Q_table)\n",
    "#     for state in Q_table:\n",
    "#         policy[state] = Q_table[state].index(max(Q_table[state]))\n",
    "#     print(policy)\n",
    "#     return policy\n",
    "    \n",
    "# def construct_path(start,goal):\n",
    "#     path = \"\"\n",
    "#     policy = create_policy()\n",
    "#     state = start \n",
    "#     path = path+f\"{state}\"\n",
    "#     while state!=goal:\n",
    "#         action = policy[state]\n",
    "#         a,b = return_index(state)\n",
    "#         next_state_index = action_reward(action,a,b)\n",
    "#         next_state = environment[(next_state_index[1],next_state_index[2])]\n",
    "#         state = next_state\n",
    "#         path = path+f\"-->{state}\"\n",
    "#     return(path)\n",
    "\n",
    "# start = input(\"Enter start state:\")\n",
    "# path = construct_path(start,goal)\n",
    "\n",
    "# print(path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a381f34",
   "metadata": {},
   "source": [
    "exclude the snippets under this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6bfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# environment = [\n",
    "#     [\"s1\",\"s2\",\"s3\"],\n",
    "#     [\"s4\",\"s5\",\"s6\"],\n",
    "#     [\"s7\",\"s8\",\"s9\"]\n",
    "# ]\n",
    "\n",
    "# def get_index(s: str):\n",
    "#     for i in range(len(environment)):\n",
    "#         for j in range(len(environment)):\n",
    "#             if environment[i][j]==s:\n",
    "#                 return (i,j)\n",
    "\n",
    "#stste : [north,east,south,west]\n",
    "# rewards = {\n",
    "#     \"s1\":[0,0,0,0],\n",
    "#     \"s2\":[0,100,0,0],\n",
    "#     \"s3\":[0,0,0,0],\n",
    "# }\n",
    "# \"S7\".lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
